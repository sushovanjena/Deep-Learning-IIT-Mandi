{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Read the univariant regression dataset files and prepare the data for training, testing, and validation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#### Path of Univariant regression dataset \n",
    "f1 = r\"D:\\Sujeet_PhD\\Course_Work\\DeepLearning (CS671)\\Assignment\\Assignment1\\dataset\\Group32\\Regression\\UnivariateData\\32.csv\"\n",
    "\n",
    "### Devide the data into training, validation, and testing data\n",
    "df = pd.read_csv(f1, header=None)\n",
    "train, validate, test = np.split(df, [int(0.6*len(df)), int(0.8*len(df))])\n",
    "trainX, trainY = train.to_numpy()[:,0].reshape((train.shape[0], 1)), train.to_numpy()[:,1].reshape((train.shape[0], 1))\n",
    "validateX, validateY = validate.to_numpy()[:,0].reshape((validate.shape[0], 1)), validate.to_numpy()[:,1].reshape((validate.shape[0], 1))\n",
    "testX, testY = test.to_numpy()[:,0].reshape((test.shape[0], 1)), test.to_numpy()[:,1].reshape((test.shape[0], 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Implementation of multilayer feed forward neural network (MLFFNN)</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "### Activation Functions Definitions\n",
    "class Sigmoid():\n",
    "    def __call__(self, x, b=1):\n",
    "        return 1.0/(1.0 + np.exp(-(b*x)))\n",
    "    def gradient(self, x, b=1):\n",
    "        return self.__call__(x, b) * (1 - self.__call__(x, b))\n",
    "\n",
    "class Linear():\n",
    "    def __call__(self, x, b=1):\n",
    "        return b*x\n",
    "    def gradient(self, x, b=1):\n",
    "        return b\n",
    "\n",
    "### Multilayer feed forward neural network class\n",
    "class MLFFNN():\n",
    "    def __init__(self, n_hidden, n_epoch=1000, learning_rate=0.01, threshold=0.001):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_epoch = n_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "        self.hidden_activation = Sigmoid()\n",
    "        self.output_activation = Linear()\n",
    "\n",
    "    ### Initialize the weights of neural network\n",
    "    def initialize_weights(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        _, n_outputs = y.shape\n",
    "        \n",
    "        ### For all hidden layers\n",
    "        pre_num_of_neuron = n_features\n",
    "        self.weights = {}\n",
    "        self.w0 = {}\n",
    "        for i in range(len(self.n_hidden)):\n",
    "            limit   = 1 / math.sqrt(pre_num_of_neuron)\n",
    "            self.weights[i]  = np.random.uniform(-limit, limit, (pre_num_of_neuron, self.n_hidden[i]))\n",
    "            self.w0[i] = np.zeros((1, self.n_hidden[i]))\n",
    "            pre_num_of_neuron = self.n_hidden[i]\n",
    "        \n",
    "        # For output layer\n",
    "        limit   = 1 / math.sqrt(pre_num_of_neuron)\n",
    "        self.V  = np.random.uniform(-limit, limit, (self.n_hidden[-1], n_outputs))\n",
    "        self.v0 = np.zeros((1, n_outputs))\n",
    "\n",
    "    def train(self, X, y, epoch=True):\n",
    "        self.initialize_weights(X, y)\n",
    "        self.errors = []\n",
    "        ### This conditional block of code is for fixed number of epoch\n",
    "        if epoch == True:\n",
    "            ### Run it for n_epoch times\n",
    "            for i in range(self.n_epoch):\n",
    "                ### For hidden layer\n",
    "                inputs = X\n",
    "                self.hidden_input = {}\n",
    "                self.hidden_output = {}\n",
    "                ### Forward Calculation ###\n",
    "                self.forward_calculation(inputs)\n",
    "                ### Backpropagation Calculation ###\n",
    "                self.backpropagation_calculation(inputs, y)\n",
    "\n",
    "                ### Store average instantaneous errors for each epoch\n",
    "                self.errors.append(np.sum(self.SquareLoss(y, self.y_pred))/y.shape[0])\n",
    "        ### This conditional block of code is for fixed threshold of average error\n",
    "        else:\n",
    "            error = 10000000\n",
    "            noOfNoChangeError = 0\n",
    "            ### Run it until error converges to the threshhold\n",
    "            while error > self.threshold:\n",
    "                ### For hidden layer\n",
    "                inputs = X\n",
    "                self.hidden_input = {}\n",
    "                self.hidden_output = {}\n",
    "                ### Forward Calculation ###\n",
    "                self.forward_calculation(inputs)\n",
    "                ### Backpropagation Calculation ###\n",
    "                self.backpropagation_calculation(inputs, y)\n",
    "                \n",
    "                ### Store average instantaneous errors for each epoch\n",
    "                self.errors.append(np.sum(self.SquareLoss(y, self.y_pred))/y.shape[0])\n",
    "                ### If there is no change in error\n",
    "                if noOfNoChangeError >=20:\n",
    "                    break\n",
    "                else:\n",
    "                    if len(self.errors) >= 4:\n",
    "                        if error == self.errors[-1] and error == self.errors[-2]:\n",
    "                            noOfNoChangeError += 1\n",
    "                error = self.errors[-1]\n",
    "                \n",
    "    ### Forward Calculation ###\n",
    "    def forward_calculation(self, inputs):\n",
    "        ### For hidden layer\n",
    "        for i in range(len(self.n_hidden)):\n",
    "            ### Input to neuron\n",
    "            self.hidden_input[i] = inputs.dot(self.weights[i]) + self.w0[i]\n",
    "            ### Output of neuron\n",
    "            self.hidden_output[i] = self.hidden_activation(self.hidden_input[i])\n",
    "            inputs = self.hidden_output[i]\n",
    "        ### For output layer\n",
    "        self.output_layer_input = inputs.dot(self.V) + self.v0\n",
    "        self.y_pred = self.output_activation(self.output_layer_input)\n",
    "        return self.y_pred\n",
    "    \n",
    "    ### Backpropagation Calculation ###\n",
    "    def backpropagation_calculation(self, inputs, y):\n",
    "        ### First for output layer\n",
    "        ### Gradient w.r.t input of output layer\n",
    "        grad_wrt_out_l_input = self.loss(y, self.y_pred) * self.output_activation.gradient(self.output_layer_input)\n",
    "        grad_v = self.hidden_output[len(self.n_hidden)-1].T.dot(grad_wrt_out_l_input)\n",
    "        grad_v0 = np.sum(grad_wrt_out_l_input, axis=0, keepdims=True)\n",
    "        ### For hidden layer\n",
    "        ### Gradient w.r.t input of hidden layer\n",
    "        next_grad_wrt_hidden_l_input = grad_wrt_out_l_input\n",
    "        next_weight = self.V\n",
    "        prev_input = inputs\n",
    "        grad_w = {}\n",
    "        grad_w0 = {}\n",
    "        ### Calculation for multiple hidden layer starting from last to first hidden layer\n",
    "        for i in reversed(range(len(self.n_hidden))):\n",
    "            grad_wrt_hidden_l_input = next_grad_wrt_hidden_l_input.dot(next_weight.T) * self.hidden_activation.gradient(self.hidden_input[i])\n",
    "            ### If hidden layer not connected to input layer\n",
    "            if i != 0:\n",
    "                grad_w[i] = self.hidden_output[i-1].T.dot(grad_wrt_hidden_l_input)\n",
    "            ### when hidden layer connected to input layer\n",
    "            else:\n",
    "                grad_w[i] = inputs.T.dot(grad_wrt_hidden_l_input)\n",
    "            grad_w0[i] = np.sum(grad_wrt_hidden_l_input, axis=0, keepdims=True)\n",
    "            next_grad_wrt_hidden_l_input = grad_wrt_hidden_l_input\n",
    "            next_weight = self.weights[i]\n",
    "\n",
    "        ### Calculaton for weights update ###\n",
    "        ### Weights update of output layer\n",
    "        self.V  -= self.learning_rate * grad_v\n",
    "        self.v0 -= self.learning_rate * grad_v0\n",
    "        ### Weights update of hidden layers\n",
    "        for i in range(len(self.n_hidden)):\n",
    "            self.weights[i]  -= self.learning_rate * grad_w[i]\n",
    "            self.w0[i] -= self.learning_rate * grad_w0[i]\n",
    "    \n",
    "    ### Prediction Function\n",
    "    def predict(self, X):\n",
    "        ### Call Forward Calculation ###\n",
    "        y_pred = self.forward_calculation(X)\n",
    "        return y_pred\n",
    "    \n",
    "    ### Instantaneous error and loss function\n",
    "    def SquareLoss(self, y, y_pred):\n",
    "        return 0.5 * np.power((y - y_pred), 2)\n",
    "    def loss(self, y, y_pred):\n",
    "        return -(y - y_pred)\n",
    "\n",
    "### Calculate the accuracy\n",
    "def accuracy_score(y, y_pred):\n",
    "    accuracy = np.sum(y == y_pred, axis=0) / len(y)\n",
    "    return accuracy*100\n",
    "\n",
    "### Plot of Epoch vs Mean Square Error\n",
    "def epochVsError_plot(model):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    error = model.errors\n",
    "    nepoch = [i+1 for i in range(len(error))]\n",
    "    plt.scatter(nepoch, error, marker='o', s=5, facecolors='b', edgecolors='b')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.ylabel('Average Error')\n",
    "#     plt.title('Average error vs number of epoch for training of Linearly Seperable data using MLFFNN')\n",
    "    plt.savefig(\"AvgErrorVsEpoch_MLFFNN_Regression1.png\", dpi=600, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "\n",
    "### Scatter plot for output of each nueron of hidden layer and output layer plot\n",
    "def output_nueron_plot(X_train, y_train, z, label='hidden', layer='', nn=''):\n",
    "    # Creating figure\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    # Add x, y gridlines \n",
    "    ax.grid(b = True, color ='grey', linestyle ='-.', linewidth = 0.3, alpha = 0.2) \n",
    "    # Creating plot\n",
    "    sctt = ax.scatter(X_train[:,0], z, s=10, c=z, cmap='Blues')\n",
    "    ax.set_xlabel('X-axis', fontweight ='bold') \n",
    "    ax.set_ylabel('Y-axis (Neuron Output)', fontweight ='bold') \n",
    "    # save plot\n",
    "    plt.savefig(\"Output_of_{}_layer_{}_Nueron_{}_Regression1.png\".format(label, layer, nn), dpi=600, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "\n",
    "def modelAndTarget_plot(target, model, label='train'):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    n_item = [i+1 for i in range(len(target))]\n",
    "    plt.scatter(n_item, model, marker='o', s=5, facecolors='b', edgecolors='b', label='Model Output')\n",
    "    plt.scatter(n_item, target, marker='s', s=5, facecolors='r', edgecolors='r', label='Target Output')\n",
    "    plt.legend(bbox_to_anchor=(0.15, 1.2), loc='upper left', ncol=2)\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Regression Output')\n",
    "#     plt.title('Model output and target output of {} dataset'.format(label))\n",
    "    plt.savefig(\"modelAndTarget_{}_MLFFNN_Regression1.png\".format(label), dpi=600, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "\n",
    "def modelVsTarget_plot(target, model, label='train'):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    plt.scatter(target, model, marker='o', s=5, facecolors='b', edgecolors='b')\n",
    "    plt.xlabel('Target Output')\n",
    "    plt.ylabel('Model Output')\n",
    "#     plt.title('Model output vs target output of {} dataset'.format(label))\n",
    "    plt.savefig(\"modelVsTarget_{}_MLFFNN_Regression1.png\".format(label), dpi=600, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "\n",
    "def MSE(target, model):\n",
    "    target = np.array(target)\n",
    "    model = np.array(model)\n",
    "    mse = np.mean(np.power(target - model, 2))\n",
    "    return mse\n",
    "\n",
    "def plot_mse(mse):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    x_data = ['train', 'validation', 'test']\n",
    "    plt.scatter(x_data, mse, marker='o', s=5, facecolors='b', edgecolors='b')\n",
    "    plt.xlabel('Type of Dataset')\n",
    "    plt.ylabel('Mean Square Error')\n",
    "#     plt.title('Mean squared error (MSE) on training data, validation data and test data')\n",
    "    plt.savefig(\"MSE_MLFFNN_Regression1.png\", dpi=600, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "    \n",
    "def main():\n",
    "    ### Call the MLFFNN calss \n",
    "    mlffnn = MLFFNN(n_hidden=[3,3], n_epoch=5000, learning_rate=0.0001, threshold=0.001)\n",
    "    ### Train the MLFFNN\n",
    "    mlffnn.train(trainX, trainY, epoch=True)\n",
    "    \n",
    "    ### Plots for output of each nueron of hidden layer and output layer plot\n",
    "    for i in range(len(mlffnn.n_hidden)):\n",
    "        for k in range(mlffnn.hidden_output[i].shape[1]):\n",
    "            z = mlffnn.hidden_output[i][:,k]\n",
    "            output_nueron_plot(trainX, trainY, z, label='hidden', layer=i+1, nn=k+1)\n",
    "    for k in range(mlffnn.y_pred.shape[1]):\n",
    "        z = mlffnn.y_pred[:,k]\n",
    "        output_nueron_plot(trainX, trainY, z, label='output', nn=k+1)\n",
    "            \n",
    "    ### Prediction for train data\n",
    "    y_pred_train = mlffnn.predict(trainX)\n",
    "    y_train = trainY\n",
    "    ### Prediction for validation data\n",
    "    y_pred_val = mlffnn.predict(validateX)\n",
    "    y_val = validateY\n",
    "    ### Prediction for test data\n",
    "    y_pred_test = mlffnn.predict(testX)\n",
    "    y_test = testY\n",
    "    \n",
    "    ### Epoch vs error plot\n",
    "    epochVsError_plot(mlffnn)\n",
    "    ### Model output and target output plot\n",
    "    modelAndTarget_plot(y_train, y_pred_train, label='train')\n",
    "    modelAndTarget_plot(y_test, y_pred_test, label='test')\n",
    "    modelAndTarget_plot(y_val, y_pred_val, label='validation')\n",
    "    ### Model output vs target output plot\n",
    "    modelVsTarget_plot(y_train, y_pred_train, label='train')\n",
    "    modelVsTarget_plot(y_test, y_pred_test, label='test')\n",
    "    modelVsTarget_plot(y_val, y_pred_val, label='validation')\n",
    "    ### For Mean square Error\n",
    "    mse_train = MSE(y_train, y_pred_train)\n",
    "    mse_val = MSE(y_val, y_pred_val)\n",
    "    mse_test = MSE(y_test, y_pred_test)\n",
    "    plot_mse([mse_train, mse_val, mse_test])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 50.22\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
