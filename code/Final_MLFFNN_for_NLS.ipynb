{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Read the non-linearly seperable classification dataset files and prepare the data for training, testing, and validation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "### Create data set by combining all three class data\n",
    "def all_class_data(train1, train2, train3, validate1, validate2, validate3, test1, test2, test3):\n",
    "    trainX = np.concatenate((train1, train2, train3), axis=0)\n",
    "    trainY = np.array([0 for i in range(len(train1))] + [1 for i in range(len(train2))] + [2 for i in range(len(train3))])\n",
    "    validateX = np.concatenate((validate1, validate2, validate3), axis=0)\n",
    "    validateY = np.array([0 for i in range(len(validate1))] + [1 for i in range(len(validate2))] + [2 for i in range(len(validate3))])\n",
    "    testX = np.concatenate((test1, test2, test3), axis=0)\n",
    "    testY = np.array([0 for i in range(len(test1))] + [1 for i in range(len(test2))] + [2 for i in range(len(test3))])\n",
    "    return trainX, trainY, validateX, validateY, testX, testY\n",
    "\n",
    "#### Path of all class dataset \n",
    "f1 = r\"D:\\Sujeet_PhD\\Course_Work\\DeepLearning (CS671)\\Assignment\\Assignment1\\dataset\\Group32\\Classification\\NLS_Group32.txt\"\n",
    "\n",
    "### Devide the Class1 data into training, validation, and testing data\n",
    "df = pd.read_csv(f1, delimiter=' ', header=None)\n",
    "df1, df2, df3 = np.split(df, [500, 1000])\n",
    "train1, validate1, test1 = np.split(df1, [int(0.6*len(df1)), int(0.8*len(df1))])\n",
    "### Devide the Class2 data into training, validation, and testing data\n",
    "train2, validate2, test2 = np.split(df2, [int(0.6*len(df2)), int(0.8*len(df2))])\n",
    "### Devide the Class3 data into training, validation, and testing data\n",
    "train3, validate3, test3 = np.split(df3, [int(0.6*len(df3)), int(0.8*len(df3))])\n",
    "\n",
    "#### Combine all class dataset to prepare training, validation, and tesing dataset ####\n",
    "trainX, trainY, validateX, validateY, testX, testY = all_class_data(train1, train2, train3, validate1, validate2, validate3, test1, test2, test3)\n",
    "\n",
    "### Convert label data to one hot encoder\n",
    "### 0 -> (1, 0, 0), 1 -> (0, 1, 0), 2 -> (0, 0, 1)\n",
    "enc = OneHotEncoder()\n",
    "y_OHE_train = enc.fit_transform(np.expand_dims(trainY,1)).toarray()\n",
    "y_OHE_val = enc.fit_transform(np.expand_dims(validateY,1)).toarray()\n",
    "y_OHE_test = enc.fit_transform(np.expand_dims(testY,1)).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Implementation of multilayer feed forward neural network (MLFFNN)</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-f8fd3f9bc155>\", line 257, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-5-f8fd3f9bc155>\", line 225, in main\n",
      "    mlffnn.train(trainX, y_OHE_train, epoch=False)\n",
      "  File \"<ipython-input-5-f8fd3f9bc155>\", line 83, in train\n",
      "    self.errors.append(np.sum(self.SquareLoss(y, self.y_pred))/y.shape[0])\n",
      "  File \"<ipython-input-5-f8fd3f9bc155>\", line 153, in SquareLoss\n",
      "    return 0.5 * np.power((y - y_pred), 2)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "### Activation Functions Definitions\n",
    "class Sigmoid():\n",
    "    def __call__(self, x, b=1):\n",
    "        return 1.0/(1.0 + np.exp(-(b*x)))\n",
    "    def gradient(self, x, b=1):\n",
    "        return self.__call__(x, b) * (1 - self.__call__(x, b))\n",
    "\n",
    "class Linear():\n",
    "    def __call__(self, x, b=1):\n",
    "        return b*x\n",
    "    def gradient(self, x, b=1):\n",
    "        return b\n",
    "\n",
    "### Multilayer feed forward neural network class\n",
    "class MLFFNN():\n",
    "    def __init__(self, n_hidden, n_epoch=1000, learning_rate=0.01, threshold=0.001):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_epoch = n_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "        self.hidden_activation = Sigmoid()\n",
    "        self.output_activation = Sigmoid()\n",
    "\n",
    "    ### Initialize the weights of neural network\n",
    "    def initialize_weights(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        _, n_outputs = y.shape\n",
    "        \n",
    "        ### For all hidden layers\n",
    "        pre_num_of_neuron = n_features\n",
    "        self.weights = {}\n",
    "        self.w0 = {}\n",
    "        for i in range(len(self.n_hidden)):\n",
    "            limit   = 1 / math.sqrt(pre_num_of_neuron/2)\n",
    "            self.weights[i]  = np.random.uniform(-limit, limit, (pre_num_of_neuron, self.n_hidden[i]))\n",
    "            self.w0[i] = np.zeros((1, self.n_hidden[i]))\n",
    "            pre_num_of_neuron = self.n_hidden[i]\n",
    "        \n",
    "        # For output layer\n",
    "        limit   = 1 / math.sqrt(pre_num_of_neuron/2)\n",
    "        self.V  = np.random.uniform(-limit, limit, (self.n_hidden[-1], n_outputs))\n",
    "        self.v0 = np.zeros((1, n_outputs))\n",
    "\n",
    "    def train(self, X, y, epoch=True):\n",
    "        self.initialize_weights(X, y)\n",
    "        self.errors = []\n",
    "        ### This conditional block of code is for fixed number of epoch\n",
    "        if epoch == True:\n",
    "            ### Run it for n_epoch times\n",
    "            for i in range(self.n_epoch):\n",
    "                ### For hidden layer\n",
    "                inputs = X\n",
    "                self.hidden_input = {}\n",
    "                self.hidden_output = {}\n",
    "                ### Forward Calculation ###\n",
    "                self.forward_calculation(inputs)\n",
    "                ### Backpropagation Calculation ###\n",
    "                self.backpropagation_calculation(inputs, y)\n",
    "\n",
    "                ### Store average instantaneous errors for each epoch\n",
    "                self.errors.append(np.sum(self.SquareLoss(y, self.y_pred))/y.shape[0])\n",
    "        ### This conditional block of code is for fixed threshold of average error\n",
    "        else:\n",
    "            error = 10000000\n",
    "            noOfNoChangeError = 0\n",
    "            ### Run it until error converges to the threshhold\n",
    "            while error > self.threshold:\n",
    "                ### For hidden layer\n",
    "                inputs = X\n",
    "                self.hidden_input = {}\n",
    "                self.hidden_output = {}\n",
    "                ### Forward Calculation ###\n",
    "                self.forward_calculation(inputs)\n",
    "                ### Backpropagation Calculation ###\n",
    "                self.backpropagation_calculation(inputs, y)\n",
    "                \n",
    "                ### Store average instantaneous errors for each epoch\n",
    "                self.errors.append(np.sum(self.SquareLoss(y, self.y_pred))/y.shape[0])\n",
    "                ### If there is no change in error\n",
    "                if noOfNoChangeError >=20:\n",
    "                    break\n",
    "                else:\n",
    "                    if len(self.errors) >= 4:\n",
    "                        if error == self.errors[-1] and error == self.errors[-2]:\n",
    "                            noOfNoChangeError += 1\n",
    "                error = self.errors[-1]\n",
    "                \n",
    "    ### Forward Calculation ###\n",
    "    def forward_calculation(self, inputs):\n",
    "        ### For hidden layer\n",
    "        for i in range(len(self.n_hidden)):\n",
    "            ### Input to neuron\n",
    "            self.hidden_input[i] = inputs.dot(self.weights[i]) + self.w0[i]\n",
    "            ### Output of neuron\n",
    "            self.hidden_output[i] = self.hidden_activation(self.hidden_input[i])\n",
    "            inputs = self.hidden_output[i]\n",
    "        ### For output layer\n",
    "        self.output_layer_input = inputs.dot(self.V) + self.v0\n",
    "        self.y_pred = self.output_activation(self.output_layer_input)\n",
    "        return self.y_pred\n",
    "    \n",
    "    ### Backpropagation Calculation ###\n",
    "    def backpropagation_calculation(self, inputs, y):\n",
    "        ### First for output layer\n",
    "        ### Gradient w.r.t input of output layer\n",
    "        grad_wrt_out_l_input = self.loss(y, self.y_pred) * self.output_activation.gradient(self.output_layer_input)\n",
    "        grad_v = self.hidden_output[len(self.n_hidden)-1].T.dot(grad_wrt_out_l_input)\n",
    "        grad_v0 = np.sum(grad_wrt_out_l_input, axis=0, keepdims=True)\n",
    "        ### For hidden layer\n",
    "        ### Gradient w.r.t input of hidden layer\n",
    "        next_grad_wrt_hidden_l_input = grad_wrt_out_l_input\n",
    "        next_weight = self.V\n",
    "        prev_input = inputs\n",
    "        grad_w = {}\n",
    "        grad_w0 = {}\n",
    "        ### Calculation for multiple hidden layer starting from last to first hidden layer\n",
    "        for i in reversed(range(len(self.n_hidden))):\n",
    "            grad_wrt_hidden_l_input = next_grad_wrt_hidden_l_input.dot(next_weight.T) * self.hidden_activation.gradient(self.hidden_input[i])\n",
    "            ### If hidden layer not connected to input layer\n",
    "            if i != 0:\n",
    "                grad_w[i] = self.hidden_output[i-1].T.dot(grad_wrt_hidden_l_input)\n",
    "            ### when hidden layer connected to input layer\n",
    "            else:\n",
    "                grad_w[i] = inputs.T.dot(grad_wrt_hidden_l_input)\n",
    "            grad_w0[i] = np.sum(grad_wrt_hidden_l_input, axis=0, keepdims=True)\n",
    "            next_grad_wrt_hidden_l_input = grad_wrt_hidden_l_input\n",
    "            next_weight = self.weights[i]\n",
    "\n",
    "        ### Calculaton for weights update ###\n",
    "        ### Weights update of output layer\n",
    "        self.V  -= self.learning_rate * grad_v\n",
    "        self.v0 -= self.learning_rate * grad_v0\n",
    "        ### Weights update of hidden layers\n",
    "        for i in range(len(self.n_hidden)):\n",
    "            self.weights[i]  -= self.learning_rate * grad_w[i]\n",
    "            self.w0[i] -= self.learning_rate * grad_w0[i]\n",
    "    \n",
    "    ### Prediction Function\n",
    "    def predict(self, X):\n",
    "        ### Call Forward Calculation ###\n",
    "        y_pred = self.forward_calculation(X)\n",
    "        return y_pred\n",
    "    \n",
    "    ### Instantaneous error and loss function\n",
    "    def SquareLoss(self, y, y_pred):\n",
    "        return 0.5 * np.power((y - y_pred), 2)\n",
    "    def loss(self, y, y_pred):\n",
    "        return -(y - y_pred)\n",
    "\n",
    "### Calculate the accuracy\n",
    "def accuracy_score(y, y_pred):\n",
    "    accuracy = np.sum(y == y_pred, axis=0) / len(y)\n",
    "    return accuracy*100\n",
    "\n",
    "### To calculate the confusion matrix and classification accuracy\n",
    "def confusion_matrix(actual, predicted):\n",
    "    cm = np.zeros((3, 3))\n",
    "    for i, j in zip(actual, predicted):\n",
    "        cm[i][j] += 1\n",
    "    ### For classification accuracy\n",
    "    accuracy = np.sum(actual == predicted) * 100.0 / float(len(actual))\n",
    "    return cm, accuracy\n",
    "\n",
    "### Plot of Epoch vs Mean Square Error\n",
    "def epochVsError_plot(model):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    error = model.errors\n",
    "    nepoch = [i+1 for i in range(len(error))]\n",
    "    plt.scatter(nepoch, error, marker='o', s=5, facecolors='b', edgecolors='b')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.ylabel('Average Error')\n",
    "#     plt.title('Average error vs number of epoch for training of Non-Linearly Seperable data using MLFFNN')\n",
    "    plt.savefig(\"AvgErrorVsEpoch_MLFFNN_NLS.png\", dpi=600, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "\n",
    "### Decision Region Plot\n",
    "def decision_boundary_plot(testX, model):\n",
    "    ### Get the minimum and maximum limit for x-axis and y-axis from data\n",
    "    x_min, x_max = testX[:, 0].min() - 1, testX[:, 0].max() + 1\n",
    "    y_min, y_max = testX[:, 1].min() - 1, testX[:, 1].max() + 1\n",
    "#     print(x_min, x_max, y_min, y_max)\n",
    "    ### Create the data points from x-axis and y-axis values with some intervals\n",
    "    h=0.05\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    g_data = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    ### prediction of created data points\n",
    "    predictedLabel = np.argmax(model.predict(g_data), axis=1)\n",
    "    colors = ['#EE6363', '#BCEE68', '#B2DFEE']\n",
    "    predictedColor = [colors[i] for i in predictedLabel]\n",
    "    \n",
    "    ### Plot input test data and decision region \n",
    "    plt.scatter(g_data[:,0], g_data[:,1], s=5, color=predictedColor)\n",
    "    plt.scatter(np.array(test1)[:,0], np.array(test1)[:,1], s=5, color='red', label='Class1')\n",
    "    plt.scatter(np.array(test2)[:,0], np.array(test2)[:,1], s=5, color='green', label='Class2')\n",
    "    plt.scatter(np.array(test3)[:,0], np.array(test3)[:,1], s=5, color='blue', label='Class3')\n",
    "    plt.legend(bbox_to_anchor=(0.05, 1.15), loc='upper left', ncol=3)\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.savefig(\"decision_boundry_MLFFNN_NLS.png\", dpi=600, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "\n",
    "### 3D scatter plot for output of each nueron of hidden layer and output layer plot\n",
    "def output_nueron_plot(X_train, y_train, z, label='hidden', layer='', nn=''):\n",
    "    # Creating figure\n",
    "    fig = plt.figure(figsize = (16, 9))\n",
    "    ax = plt.axes(projection =\"3d\")\n",
    "    # Add x, y gridlines \n",
    "    ax.grid(b = True, color ='grey', linestyle ='-.', linewidth = 0.3, alpha = 0.2) \n",
    "    # Creating color map\n",
    "    color = ['red', 'green', 'blue']\n",
    "    color_list = [color[i] for i in y_train]\n",
    "    # Creating plot\n",
    "    sctt = ax.scatter3D(X_train[:,0], X_train[:,1], z, color = color_list)\n",
    "    ax.set_xlabel('X-axis', fontweight ='bold') \n",
    "    ax.set_ylabel('Y-axis', fontweight ='bold') \n",
    "    ax.set_zlabel('Z-axis (Neuron Output) ', fontweight ='bold')\n",
    "    # save plot\n",
    "    plt.savefig(\"Output_of_{}_layer_{}_Nueron_{}_NLS.png\".format(label, layer, nn), dpi=600, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "\n",
    "def main():\n",
    "    ### Call the MLFFNN calss \n",
    "    mlffnn = MLFFNN(n_hidden=[3,3], n_epoch=1000, learning_rate=0.01, threshold=0.001)\n",
    "    ### Train the MLFFNN\n",
    "    mlffnn.train(trainX, y_OHE_train, epoch=False)\n",
    "    \n",
    "    ### Plots for output of each nueron of hidden layer and output layer plot\n",
    "    for i in range(len(mlffnn.n_hidden)):\n",
    "        for k in range(mlffnn.hidden_output[i].shape[1]):\n",
    "            z = mlffnn.hidden_output[i][:,k]\n",
    "            output_nueron_plot(trainX, trainY, z, label='hidden', layer=i+1, nn=k+1)\n",
    "    for k in range(mlffnn.y_pred.shape[1]):\n",
    "        z = mlffnn.y_pred[:,k]\n",
    "        output_nueron_plot(trainX, trainY, z, label='output', nn=k+1)\n",
    "            \n",
    "    ### Prediction for validation data\n",
    "    y_pred_val = np.argmax(mlffnn.predict(validateX), axis=1)\n",
    "    y_val = np.argmax(y_OHE_val, axis=1)\n",
    "    \n",
    "    ### Prediction for test data\n",
    "    y_pred_test = np.argmax(mlffnn.predict(testX), axis=1)\n",
    "    y_test = np.argmax(y_OHE_test, axis=1)\n",
    "\n",
    "    ### Calculate the accuracy and confusion matrix for validate and test data\n",
    "    CM, Accuracy = confusion_matrix(y_val, y_pred_val)\n",
    "    print(\"Confusion Matrix of Validate data: {}\".format(CM))\n",
    "    print(\"Classification Accuracy of Validate data: {}\".format(Accuracy))\n",
    "    ### Call the confusion_matrix function\n",
    "    CM, Accuracy = confusion_matrix(y_test, y_pred_test)\n",
    "    print(\"Confusion Matrix of Validate data: {}\".format(CM))\n",
    "    print(\"Classification Accuracy of Validate data: {}\".format(Accuracy))\n",
    "\n",
    "    ### Epoch vs error plot\n",
    "    epochVsError_plot(mlffnn)\n",
    "    \n",
    "    ### Decision boundary plot\n",
    "    decision_boundary_plot(testX, mlffnn)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
